{
    "nbformat_minor": 1, 
    "cells": [
        {
            "source": "# Spark ML package\n* This is an introduction to machine learning library in spark. \n* There are two implementations in spark. note the differences below.\n    - Spark Mllib\n        - spark.mllib contains the legacy API built on top of RDDs.\n    - Spark ML\n        - spark.ml provides higher-level API built on top of DataFrames for constructing ML pipelines.\n        - very similar to sklearn", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": 3, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "+---+---+---+--------------------+------------+\n|  x|  y|  z|              source|       class|\n+---+---+---+--------------------+------------+\n| 30| 36| 52|Accelerometer-201...|Climb_stairs|\n| 30| 36| 32|Accelerometer-201...|Climb_stairs|\n| 32| 30| 36|Accelerometer-201...|Climb_stairs|\n| 32| 30| 36|Accelerometer-201...|Climb_stairs|\n| 32| 30| 36|Accelerometer-201...|Climb_stairs|\n| 32| 33| 36|Accelerometer-201...|Climb_stairs|\n| 32| 30| 36|Accelerometer-201...|Climb_stairs|\n| 30| 37| 50|Accelerometer-201...|Climb_stairs|\n| 31| 37| 50|Accelerometer-201...|Climb_stairs|\n| 29| 38| 50|Accelerometer-201...|Climb_stairs|\n| 32| 39| 48|Accelerometer-201...|Climb_stairs|\n| 32| 41| 46|Accelerometer-201...|Climb_stairs|\n| 34| 39| 44|Accelerometer-201...|Climb_stairs|\n| 34| 39| 41|Accelerometer-201...|Climb_stairs|\n| 31| 39| 42|Accelerometer-201...|Climb_stairs|\n| 32| 42| 43|Accelerometer-201...|Climb_stairs|\n| 31| 43| 43|Accelerometer-201...|Climb_stairs|\n| 28| 46| 44|Accelerometer-201...|Climb_stairs|\n| 26| 48| 43|Accelerometer-201...|Climb_stairs|\n| 22| 41| 40|Accelerometer-201...|Climb_stairs|\n+---+---+---+--------------------+------------+\nonly showing top 20 rows\n\n"
                }
            ], 
            "source": "# The code was removed by Watson Studio for sharing."
        }, 
        {
            "source": "## pyspark.ml.feature\n* StringIndexer\n    -  maps a string column of labels to an ML column of label indices.If the input column is numeric, we cast it to string and index the string values. The indices are in [0, numLabels). By default, this is ordered by label frequencies so the most frequent label gets index 0.\n    \n* OneHotEncoder\n    - maps a column of category indices to a column of binary vectors. The last category is not included by default (configurable via dropLast) to avoid colinearity.\n    \n* VectorAssembler\n    - merges multiple columns into a vector column.\n    \n* Normalizer\n    - Normalize a vector to have unit norm using the given p-norm.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "### StringIndexer", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": 4, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "+---+---+---+--------------------+------------+----------+\n|  x|  y|  z|              source|       class|classIndex|\n+---+---+---+--------------------+------------+----------+\n| 30| 36| 52|Accelerometer-201...|Climb_stairs|       3.0|\n| 30| 36| 32|Accelerometer-201...|Climb_stairs|       3.0|\n| 32| 30| 36|Accelerometer-201...|Climb_stairs|       3.0|\n| 32| 30| 36|Accelerometer-201...|Climb_stairs|       3.0|\n| 32| 30| 36|Accelerometer-201...|Climb_stairs|       3.0|\n| 32| 33| 36|Accelerometer-201...|Climb_stairs|       3.0|\n| 32| 30| 36|Accelerometer-201...|Climb_stairs|       3.0|\n| 30| 37| 50|Accelerometer-201...|Climb_stairs|       3.0|\n| 31| 37| 50|Accelerometer-201...|Climb_stairs|       3.0|\n| 29| 38| 50|Accelerometer-201...|Climb_stairs|       3.0|\n| 32| 39| 48|Accelerometer-201...|Climb_stairs|       3.0|\n| 32| 41| 46|Accelerometer-201...|Climb_stairs|       3.0|\n| 34| 39| 44|Accelerometer-201...|Climb_stairs|       3.0|\n| 34| 39| 41|Accelerometer-201...|Climb_stairs|       3.0|\n| 31| 39| 42|Accelerometer-201...|Climb_stairs|       3.0|\n| 32| 42| 43|Accelerometer-201...|Climb_stairs|       3.0|\n| 31| 43| 43|Accelerometer-201...|Climb_stairs|       3.0|\n| 28| 46| 44|Accelerometer-201...|Climb_stairs|       3.0|\n| 26| 48| 43|Accelerometer-201...|Climb_stairs|       3.0|\n| 22| 41| 40|Accelerometer-201...|Climb_stairs|       3.0|\n+---+---+---+--------------------+------------+----------+\nonly showing top 20 rows\n\n+----------+\n|classIndex|\n+----------+\n|       8.0|\n|       0.0|\n|       7.0|\n|       1.0|\n|       4.0|\n|      11.0|\n|       3.0|\n|       2.0|\n|      10.0|\n|       6.0|\n|       5.0|\n|       9.0|\n|      12.0|\n+----------+\n\n"
                }
            ], 
            "source": "from pyspark.ml.feature import StringIndexer\n\nindexer = StringIndexer(inputCol=\"class\", outputCol=\"classIndex\")\nindexed = indexer.fit(df).transform(df)\nindexed.show()\nindexed.select('classIndex').distinct().show()"
        }, 
        {
            "source": "### OneHotEncoder", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": 5, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "+---+---+---+--------------------+------------+----------+--------------+\n|  x|  y|  z|              source|       class|classIndex|   categoryVec|\n+---+---+---+--------------------+------------+----------+--------------+\n| 30| 36| 52|Accelerometer-201...|Climb_stairs|       3.0|(12,[3],[1.0])|\n| 30| 36| 32|Accelerometer-201...|Climb_stairs|       3.0|(12,[3],[1.0])|\n| 32| 30| 36|Accelerometer-201...|Climb_stairs|       3.0|(12,[3],[1.0])|\n| 32| 30| 36|Accelerometer-201...|Climb_stairs|       3.0|(12,[3],[1.0])|\n| 32| 30| 36|Accelerometer-201...|Climb_stairs|       3.0|(12,[3],[1.0])|\n| 32| 33| 36|Accelerometer-201...|Climb_stairs|       3.0|(12,[3],[1.0])|\n| 32| 30| 36|Accelerometer-201...|Climb_stairs|       3.0|(12,[3],[1.0])|\n| 30| 37| 50|Accelerometer-201...|Climb_stairs|       3.0|(12,[3],[1.0])|\n| 31| 37| 50|Accelerometer-201...|Climb_stairs|       3.0|(12,[3],[1.0])|\n| 29| 38| 50|Accelerometer-201...|Climb_stairs|       3.0|(12,[3],[1.0])|\n| 32| 39| 48|Accelerometer-201...|Climb_stairs|       3.0|(12,[3],[1.0])|\n| 32| 41| 46|Accelerometer-201...|Climb_stairs|       3.0|(12,[3],[1.0])|\n| 34| 39| 44|Accelerometer-201...|Climb_stairs|       3.0|(12,[3],[1.0])|\n| 34| 39| 41|Accelerometer-201...|Climb_stairs|       3.0|(12,[3],[1.0])|\n| 31| 39| 42|Accelerometer-201...|Climb_stairs|       3.0|(12,[3],[1.0])|\n| 32| 42| 43|Accelerometer-201...|Climb_stairs|       3.0|(12,[3],[1.0])|\n| 31| 43| 43|Accelerometer-201...|Climb_stairs|       3.0|(12,[3],[1.0])|\n| 28| 46| 44|Accelerometer-201...|Climb_stairs|       3.0|(12,[3],[1.0])|\n| 26| 48| 43|Accelerometer-201...|Climb_stairs|       3.0|(12,[3],[1.0])|\n| 22| 41| 40|Accelerometer-201...|Climb_stairs|       3.0|(12,[3],[1.0])|\n+---+---+---+--------------------+------------+----------+--------------+\nonly showing top 20 rows\n\n"
                }
            ], 
            "source": "from pyspark.ml.feature import OneHotEncoder\nencoder = OneHotEncoder(inputCol=\"classIndex\", outputCol=\"categoryVec\")\nencoded = encoder.transform(indexed)\nencoded.show()"
        }, 
        {
            "execution_count": 7, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "+----------+---------------+\n|classIndex|    categoryVec|\n+----------+---------------+\n|       8.0| (12,[8],[1.0])|\n|       4.0| (12,[4],[1.0])|\n|      12.0|     (12,[],[])|\n|       7.0| (12,[7],[1.0])|\n|       1.0| (12,[1],[1.0])|\n|       0.0| (12,[0],[1.0])|\n|       5.0| (12,[5],[1.0])|\n|       2.0| (12,[2],[1.0])|\n|      10.0|(12,[10],[1.0])|\n|      11.0|(12,[11],[1.0])|\n|       6.0| (12,[6],[1.0])|\n|       3.0| (12,[3],[1.0])|\n|       9.0| (12,[9],[1.0])|\n+----------+---------------+\n\n"
                }
            ], 
            "source": "encoded.select('classIndex','categoryVec').distinct().show()   "
        }, 
        {
            "source": "### VectorAssembler", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": 8, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "+---+---+---+--------------------+------------+----------+--------------+----------------+\n|  x|  y|  z|              source|       class|classIndex|   categoryVec|        features|\n+---+---+---+--------------------+------------+----------+--------------+----------------+\n| 30| 36| 52|Accelerometer-201...|Climb_stairs|       3.0|(12,[3],[1.0])|[30.0,36.0,52.0]|\n| 30| 36| 32|Accelerometer-201...|Climb_stairs|       3.0|(12,[3],[1.0])|[30.0,36.0,32.0]|\n| 32| 30| 36|Accelerometer-201...|Climb_stairs|       3.0|(12,[3],[1.0])|[32.0,30.0,36.0]|\n| 32| 30| 36|Accelerometer-201...|Climb_stairs|       3.0|(12,[3],[1.0])|[32.0,30.0,36.0]|\n| 32| 30| 36|Accelerometer-201...|Climb_stairs|       3.0|(12,[3],[1.0])|[32.0,30.0,36.0]|\n+---+---+---+--------------------+------------+----------+--------------+----------------+\nonly showing top 5 rows\n\n"
                }
            ], 
            "source": "from pyspark.ml.feature import VectorAssembler\n\nvectorAssembler = VectorAssembler(inputCols=[\"x\",\"y\",\"z\"],outputCol=\"features\")\nfeatures_vectorized = vectorAssembler.transform(encoded)\nfeatures_vectorized.show(5)"
        }, 
        {
            "source": "### Normalizer", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": 10, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "+---+---+---+--------------------+------------+----------+--------------+----------------+--------------------+\n|  x|  y|  z|              source|       class|classIndex|   categoryVec|        features|       features_norm|\n+---+---+---+--------------------+------------+----------+--------------+----------------+--------------------+\n| 30| 36| 52|Accelerometer-201...|Climb_stairs|       3.0|(12,[3],[1.0])|[30.0,36.0,52.0]|[0.25423728813559...|\n| 30| 36| 32|Accelerometer-201...|Climb_stairs|       3.0|(12,[3],[1.0])|[30.0,36.0,32.0]|[0.30612244897959...|\n| 32| 30| 36|Accelerometer-201...|Climb_stairs|       3.0|(12,[3],[1.0])|[32.0,30.0,36.0]|[0.32653061224489...|\n| 32| 30| 36|Accelerometer-201...|Climb_stairs|       3.0|(12,[3],[1.0])|[32.0,30.0,36.0]|[0.32653061224489...|\n| 32| 30| 36|Accelerometer-201...|Climb_stairs|       3.0|(12,[3],[1.0])|[32.0,30.0,36.0]|[0.32653061224489...|\n+---+---+---+--------------------+------------+----------+--------------+----------------+--------------------+\nonly showing top 5 rows\n\n"
                }
            ], 
            "source": "from pyspark.ml.feature import Normalizer\n\nnormalizer = Normalizer(inputCol=\"features\", outputCol=\"features_norm\", p=1.0)\nl1NormData = normalizer.transform(features_vectorized)\nl1NormData.show(5)"
        }, 
        {
            "execution_count": 11, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "+--------------+--------------------+\n|   categoryVec|       features_norm|\n+--------------+--------------------+\n|(12,[3],[1.0])|[0.25423728813559...|\n|(12,[3],[1.0])|[0.30612244897959...|\n|(12,[3],[1.0])|[0.32653061224489...|\n|(12,[3],[1.0])|[0.32653061224489...|\n|(12,[3],[1.0])|[0.32653061224489...|\n+--------------+--------------------+\nonly showing top 5 rows\n\n"
                }
            ], 
            "source": "df_train = l1NormData.drop(\"source\").drop(\"class\").drop(\"classIndex\").drop(\"features\").drop(\"x\").drop(\"y\").drop(\"z\")\ndf_train.show(5)\n"
        }, 
        {
            "source": "### combine all preprocessing together", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": 13, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "+---+---+---+--------------------+------------+----------+--------------+----------------+--------------------+\n|  x|  y|  z|              source|       class|classIndex|   categoryVec|        features|       features_norm|\n+---+---+---+--------------------+------------+----------+--------------+----------------+--------------------+\n| 30| 36| 52|Accelerometer-201...|Climb_stairs|       3.0|(12,[3],[1.0])|[30.0,36.0,52.0]|[0.25423728813559...|\n| 30| 36| 32|Accelerometer-201...|Climb_stairs|       3.0|(12,[3],[1.0])|[30.0,36.0,32.0]|[0.30612244897959...|\n| 32| 30| 36|Accelerometer-201...|Climb_stairs|       3.0|(12,[3],[1.0])|[32.0,30.0,36.0]|[0.32653061224489...|\n| 32| 30| 36|Accelerometer-201...|Climb_stairs|       3.0|(12,[3],[1.0])|[32.0,30.0,36.0]|[0.32653061224489...|\n| 32| 30| 36|Accelerometer-201...|Climb_stairs|       3.0|(12,[3],[1.0])|[32.0,30.0,36.0]|[0.32653061224489...|\n+---+---+---+--------------------+------------+----------+--------------+----------------+--------------------+\nonly showing top 5 rows\n\n"
                }
            ], 
            "source": "from pyspark.ml import Pipeline\npipeline = Pipeline(stages=[indexer, encoder, vectorAssembler, normalizer])\nmodel = pipeline.fit(df)\nprediction = model.transform(df)\nprediction.show(5)"
        }, 
        {
            "source": "## spark ML models\n### Classification\n* LinearSVC: new in 2.2\n    - only support L2 regularization\n    - optimize Hinge loss\n    - only binary\n* LogisticRegression\n* DecisionTreeClassifier\n* GBTClassifier - binary only\n* RandomForestClassifier\n* NaiveBayes\n", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": 80, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "from pyspark.ml.feature import StringIndexer\n\nindexer = StringIndexer(inputCol=\"class\", outputCol=\"classIndex\")\ndfindexed = indexer.fit(df).transform(df)\n# dfindexed = dfindexed.where('classIndex==0 or classIndex==1')\n# rename column\ndfindexed = dfindexed.withColumnRenamed('classIndex','label')\ntraining_df,test_df=dfindexed.randomSplit([0.75,0.25])"
        }, 
        {
            "execution_count": 81, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "\nfrom pyspark.ml.feature import VectorAssembler\nfrom pyspark.ml.feature import Normalizer\n\nvectorAssembler = VectorAssembler(inputCols=['x','y','z'],outputCol='features')\nnormalizer = Normalizer(inputCol='features',outputCol='features_norm',p=1)\nfrom pyspark.ml.classification import LogisticRegression\nlr = LogisticRegression(maxIter=10,featuresCol='features_norm')"
        }, 
        {
            "execution_count": 86, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "from pyspark.ml.classification import DecisionTreeClassifier\ndt = DecisionTreeClassifier(maxDepth=5, featuresCol='features_norm')\nfrom pyspark.ml.classification import GBTClassifier\ngbt = GBTClassifier(maxIter=2,maxDepth=3,featuresCol='features_norm')\nfrom pyspark.ml.classification import RandomForestClassifier\nrf = RandomForestClassifier(numTrees=3,maxDepth=5,featuresCol='features_norm')\nfrom pyspark.ml.classification import NaiveBayes\nnb = NaiveBayes(featuresCol='features_norm')"
        }, 
        {
            "execution_count": 87, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "0.2946619855753592\n0.3829147645059617\n0.3865601830560946\n0.13043868405998676\n"
                }
            ], 
            "source": "from pyspark.ml import Pipeline\nfrom pyspark.ml.evaluation import MulticlassClassificationEvaluator\n\nfor fitmodel in [lr,dt,rf,nb]:\n    pipeline = Pipeline(stages=[vectorAssembler,normalizer,fitmodel])\n    model = pipeline.fit(training_df)\n    pred = model.transform(test_df)\n\n    evaluator = MulticlassClassificationEvaluator(predictionCol=\"prediction\")\n    print(evaluator.evaluate(pred,{evaluator.metricName: \"accuracy\"}))"
        }, 
        {
            "source": "### Regression\n* LinearRegression \n* DecisionTreeRegressor - supports both continuous and categorical features.\n* GeneralizedLinearRegression\n* GBTRegressor\n* RandomForest", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": ""
        }
    ], 
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3.6 with Spark", 
            "name": "python36", 
            "language": "python3"
        }, 
        "language_info": {
            "mimetype": "text/x-python", 
            "nbconvert_exporter": "python", 
            "version": "3.6.8", 
            "name": "python", 
            "file_extension": ".py", 
            "pygments_lexer": "ipython3", 
            "codemirror_mode": {
                "version": 3, 
                "name": "ipython"
            }
        }
    }, 
    "nbformat": 4
}